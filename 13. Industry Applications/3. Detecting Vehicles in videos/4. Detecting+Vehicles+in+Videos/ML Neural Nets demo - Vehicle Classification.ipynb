{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "twoW 0\n",
    "threeFourW 1\n",
    "fivePlusW 2\n",
    "'''\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "video = \"Aundh\" #file name of npz\n",
    "mode = \"test\"\n",
    "file_name = \"Aundh.npz\"\n",
    "to_stack = False\n",
    "\n",
    "# Let's define a dictionary that maps the different classes we have to numbers\n",
    "\n",
    "dict = {'twoW':0, 'threeFourW':1, 'fivePlusW':2}\n",
    "for vehicle in sorted(dict):\n",
    "\t#vehicle = \"twowheeler\"\n",
    "\t\n",
    "\tcount = 0\n",
    "\tarr_stack = np.zeros((1, 50, 50, 3))\n",
    "\ty = dict[vehicle] * np.ones((1, 1))\n",
    "\ts = np.array([dict[vehicle]])\n",
    "\ts = s[np.newaxis, :]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\tfor i in range(100):\n",
    "\t\tpath = \"./saved_images/\" \n",
    "\t\t#path = \"/home/chinmays/win/Users/Chinmay/Documents/vehicleClassification_Data/Dataset/\" + video + \"/\" + \"resized/\" + vehicle + \"/\"\n",
    "\t\t#img_path = path + video + \"_\" + vehicle + \"_\" + str(i) + \".jpg\"\n",
    "\t\timg_path = path + str(i) + \".jpg\"\t\n",
    "\t\ttry:\t\n",
    "\t\t\timg = Image.open(img_path)\n",
    "\t\t\tif count == 0:\n",
    "\t\t\t\tarr = np.array(img)\n",
    "\t\t\t\tarr_stack[0, :, :, :] = arr\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\telse: \n",
    "\t\t\t\tarr = np.array(img)\n",
    "\t\t\t\tarr_stack = np.concatenate((arr_stack, arr[np.newaxis, ...]), axis = 0)\n",
    "\t\t\t\ty = np.concatenate((y, s), axis = 0)\n",
    "\t\t\t\t# print arr_stack.shape, s\n",
    "\t\texcept IOError:\n",
    "\t\t\tprint (\"Error\")\n",
    "\n",
    "\tif to_stack:\n",
    "\t\tarr_test = np.load(file_name)\n",
    "\t\tarr_stack = np.concatenate((arr_stack, arr_test['a']), axis = 0)\n",
    "\t\ty = np.concatenate((y, arr_test['b']), axis = 0)\n",
    "\n",
    "\tnp.savez_compressed(video, a = arr_stack, b = y)\n",
    "\tto_stack = True\n",
    "arr_test = np.load(file_name)\n",
    "# print 'ff', arr_test['a'].shape, arr_test['b'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trainAundh.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e502a88b90a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mimg_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainAundh.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainAundh.npz'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training Code\n",
    "Save model after compiling it, in case you have to press ^C during the execution of fit function.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "#from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 3\n",
    "nb_epoch = 300\n",
    "data_augmentation = True\n",
    "\n",
    "img_rows, img_cols = 50, 50\n",
    "img_channels = 3\n",
    "\n",
    "arr = numpy.load(\"trainAundh.npz\")\n",
    "X_train = 255 - arr['a']\n",
    "y_train = arr['b']\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "arr = numpy.load(\"testAundh.npz\")\n",
    "X_test = 255 - arr['a']\n",
    "y_test = arr['b']\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train -= 0.5\n",
    "X_test -= 0.5\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_rows, img_cols, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, W_regularizer=l2(0.1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, W_regularizer=l2(0.1)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#HERE: Save model after compiling it, in case you have to press ^C during the execution of fit function.\n",
    "model_json = model.to_json()\n",
    "with open(\"modelAundh.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "filename = \"modelAundh.h5\"\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [checkpoint]\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "\t  callbacks=callback_list,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code to check accuracies of trained models on any dataset.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "arr = numpy.load(\"trainAundh.npz\")\n",
    "X_train = 255 - arr['a']\n",
    "y_train = arr['b']\n",
    "\n",
    "#Load the dataset for which you want to get the accuracy.\n",
    "arr = numpy.load(\"testAundh.npz\")\n",
    "X_test = 255 - arr['a']\n",
    "y_test = arr['b']\n",
    "X_display = numpy.copy(X_test)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train -= 0.5\n",
    "X_test -= 0.5\n",
    "\n",
    "path = '../data/'\n",
    "model_name = 'modelAundh.json'\n",
    "\n",
    "json_file = open(model_name, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "\t      optimizer='adam',\n",
    "\t      metrics=['accuracy'])\n",
    "print(\"CNN Loaded\")\n",
    "loaded_model.summary()\n",
    "#Iterate over all the weights, run a forward pass, check accuracy\n",
    "for i in range(1, 6):\n",
    "\tweights_name = 'modelAundh.h5'\n",
    "\tloaded_model.load_weights(weights_name)\n",
    "\tall_predictions = loaded_model.predict(X_test, batch_size = 32, verbose = 0) #Gives class probabilities\n",
    "\tall_predictions = numpy.argmax(all_predictions, axis = 1) #Finds max probability. That is the output class of the image.\n",
    "\tall_predictions = all_predictions[:, numpy.newaxis] #Reshape to y_test.shape\n",
    "\terror = all_predictions == y_test #Find correctly classified images\n",
    "\tacc = float(numpy.sum(error))\n",
    "\tacc /= y_test.shape[0]\n",
    "\tacc *= 100\n",
    "\tprint('accuracy', acc)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Image and its class, optionally display accuracy.\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def decode_predictions(predictions, images):\n",
    "\t\n",
    "\t# Function to display image and its class.\n",
    "\t\t\n",
    "\tdict_map = {0:'twoW', 1:'threeFourW', 2:'fivePlusW'}\n",
    "\tnImages = predictions.shape[0]\n",
    "\tprint (nImages)\n",
    "\tfor i in range(nImages):\n",
    "\t\tprint(dict_map[predictions[i]] + \" detected\")\n",
    "\t\tplt.imshow(Image.fromarray(images[i]))\n",
    "\t\tplt.show()\n",
    "\n",
    "arr = numpy.load(\"trainAundh.npz\")\n",
    "X_train = 255 - arr['a']\n",
    "y_train = arr['b']\n",
    "\n",
    "#Change the file name if you want to load any other dataset.\n",
    "arr = numpy.load(\"testAundh.npz\")\n",
    "X_test = 255 - arr['a']\n",
    "X_display = arr['a']\n",
    "print('X_test', X_test.shape)\n",
    "y_test = arr['b']\n",
    "print('y_test', y_test.shape)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train -= 0.5\n",
    "X_test -= 0.5\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "json_file = open('modelAundh.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"modelAundh.h5\")\n",
    "print(\"CNN Loaded\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Uncomment the block below to display images and its class\n",
    "\n",
    "rng = numpy.arange(1988, 1998)\n",
    "X_toPredict = X_test[rng, :, :, :]\n",
    "\n",
    "predictions = loaded_model.predict(X_toPredict, batch_size = 32, verbose = 0)\n",
    "predictions = numpy.argmax(predictions, axis = 1)\n",
    "print('predictions', predictions)\n",
    "print(predictions[:, numpy.newaxis].shape, y_test.shape)\n",
    "decode_predictions(predictions, X_display[rng, :, :, :])\n",
    "\n",
    "\n",
    "#Checks the accuracy on the curent test dataset, by using the method described in get_accuracy.py\n",
    "all_predictions = loaded_model.predict(X_test, batch_size = 32, verbose = 0)\n",
    "all_predictions = numpy.argmax(all_predictions, axis = 1)\n",
    "all_predictions = all_predictions[:, numpy.newaxis]\n",
    "error = all_predictions == y_test\n",
    "acc = float(numpy.sum(error))\n",
    "acc /= y_test.shape[0]\n",
    "acc *= 100\n",
    "print('accuracy', acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
