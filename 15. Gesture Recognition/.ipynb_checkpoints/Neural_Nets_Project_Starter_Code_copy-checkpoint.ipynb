{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.transform import resize as imresize\n",
    "from matplotlib.pyplot import imread\n",
    "# from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(663, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>Type</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIN_20180925_17_08_43_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WIN_20180925_17_18_28_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WIN_20180925_17_18_56_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WIN_20180925_17_19_51_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WIN_20180925_17_20_14_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Folder            Type  Tag\n",
       "0  WIN_20180925_17_08_43_Pro_Left_Swipe_new  Left_Swipe_new    0\n",
       "1  WIN_20180925_17_18_28_Pro_Left_Swipe_new  Left_Swipe_new    0\n",
       "2  WIN_20180925_17_18_56_Pro_Left_Swipe_new  Left_Swipe_new    0\n",
       "3  WIN_20180925_17_19_51_Pro_Left_Swipe_new  Left_Swipe_new    0\n",
       "4  WIN_20180925_17_20_14_Pro_Left_Swipe_new  Left_Swipe_new    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = pd.read_csv('Project_data/train.csv', sep=';', names=['Folder', 'Type', 'Tag'])\n",
    "print(td.shape)\n",
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "#experiment with the batch size\n",
    "batch_size = 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WIN_20180926_17_24_20_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180907_15_54_30_Pro_Thumbs Up_new;Thumbs Up_new;4\\n',\n",
       "       'WIN_20180926_17_08_11_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180925_18_01_40_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_16_16_48_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180907_16_33_15_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180926_16_46_22_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_16_25_44_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180926_17_05_38_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180907_15_55_06_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180926_16_57_50_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180926_17_24_12_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180907_15_57_43_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180926_17_05_38_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180926_17_34_23_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180925_17_50_24_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180925_17_40_03_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180907_16_25_57_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_48_16_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180907_15_47_33_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180925_18_03_21_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180926_17_33_49_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180926_17_04_53_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180926_16_47_09_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_16_29_12_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180907_15_45_43_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180926_16_48_48_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180925_17_25_06_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180907_16_07_10_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180925_17_17_04_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_16_02_09_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180907_15_30_06_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180926_17_40_21_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180926_16_44_04_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180907_16_13_24_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180926_17_08_09_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_15_38_42_Pro_Thumbs Up_new;Thumbs Up_new;4\\n',\n",
       "       'WIN_20180926_17_32_55_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180926_17_33_14_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180907_16_10_59_Pro_Thumbs Up_new;Thumbs Up_new;4\\n',\n",
       "       'WIN_20180907_16_21_39_Pro_Left Swipe_new_Left Swipe_new;Left Swipe_new_Left Swipe_new;0\\n',\n",
       "       'WIN_20180926_17_34_05_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180907_16_30_54_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180926_17_33_14_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180925_17_52_42_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180926_17_09_45_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180926_17_01_52_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180926_16_43_34_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180907_16_05_32_Pro_Left Swipe_new_Left Swipe_new;Left Swipe_new_Left Swipe_new;0\\n',\n",
       "       'WIN_20180907_15_52_05_Pro_Thumbs Up_new;Thumbs Up_new;4\\n',\n",
       "       'WIN_20180907_15_43_36_Pro_Thumbs Up_new;Thumbs Up_new;4\\n',\n",
       "       'WIN_20180907_16_20_15_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180926_17_28_32_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180907_16_09_35_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180907_16_36_42_Pro_Left Swipe_new_Left Swipe_new;Left Swipe_new_Left Swipe_new;0\\n',\n",
       "       'WIN_20180907_15_42_17_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180926_17_14_26_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180907_16_18_23_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180925_17_38_43_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180907_16_20_53_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180926_17_32_15_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180925_17_35_04_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180907_15_45_04_Pro_Left Swipe_new_Left Swipe_new;Left Swipe_new_Left Swipe_new;0\\n',\n",
       "       'WIN_20180926_16_47_44_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180907_16_14_40_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_32_32_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180925_17_43_01_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180926_17_03_57_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180907_15_42_25_Pro_Thumbs Down_new;Thumbs Down_new;3\\n',\n",
       "       'WIN_20180926_17_32_40_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180907_16_14_16_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180907_16_05_10_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_33_30_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180926_17_09_33_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_15_50_39_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_58_08_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180925_17_59_48_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180907_16_02_38_Pro_Right Swipe_new;Right Swipe_new;1\\n',\n",
       "       'WIN_20180926_16_37_56_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180926_17_29_34_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_35_29_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180926_17_35_34_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180926_17_21_48_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180907_16_00_42_Pro_Left Swipe_new_Left Swipe_new;Left Swipe_new_Left Swipe_new;0\\n',\n",
       "       'WIN_20180907_16_39_59_Pro_Thumbs Up_new;Thumbs Up_new;4\\n',\n",
       "       'WIN_20180926_17_06_40_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180926_17_12_27_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180926_17_44_14_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180907_16_17_35_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180925_17_47_07_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180926_17_56_52_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180907_16_31_41_Pro_Stop Gesture_new;Stop Gesture_new;2\\n',\n",
       "       'WIN_20180925_17_42_34_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n',\n",
       "       'WIN_20180926_17_15_35_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180926_16_48_40_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_30_40_Pro_Stop_new;Stop_new;2\\n',\n",
       "       'WIN_20180926_17_13_25_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180925_17_43_46_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180926_16_52_49_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180925_17_49_40_Pro_Stop_new;Stop_new;2\\n'], dtype='<U88')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_list : train_doc/val_doc\n",
    "# source_path : root dir where our data is available\n",
    "\n",
    "# total number of frames for a particular video\n",
    "x = 30 \n",
    "\n",
    "# width of the image\n",
    "y = 100 \n",
    "\n",
    "# height of the image\n",
    "z = 100 \n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    # using all the images for a particular video\n",
    "    img_idx = [i for i in range(0, x)] \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        \n",
    "        # calculate the number of batches\n",
    "        num_batches = len(folder_list)//batch_size \n",
    "        \n",
    "        # we iterate over the number of batches\n",
    "        for batch in range(num_batches): \n",
    "            ## x is the number of images you use for each video, (y,z) is the final size of the input images \n",
    "            # and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size, x, y, z, 3)) \n",
    "            \n",
    "            # batch_labels is the one hot representation of the output\n",
    "            batch_labels = np.zeros((batch_size, 5)) \n",
    "            # iterate over the batch_size\n",
    "            for folder in range(batch_size): \n",
    "                \n",
    "                # read all the images inside the folder\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "                \n",
    "                # Iterate over the frames/images of a folder to read them in\n",
    "                for idx, item in enumerate(img_idx): \n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+\n",
    "                                   imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resizedImage = imresize(image, (y, z))\n",
    "#                     norm_img = np.zeros((y, z))\n",
    "#                     resizedImage = cv.normalize(resizedImage,  norm_img, 0, 255, cv.NORM_MINMAX)\n",
    "                    batch_data[folder, idx, :, :, 0] = resizedImage[:,:,0]\n",
    "                    batch_data[folder, idx, :, :, 1] = resizedImage[:,:,1]\n",
    "                    batch_data[folder, idx, :, :, 2] = resizedImage[:,:,2]\n",
    "#                     print(batch_data.shape)\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 50 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(generator(train_path, train_doc, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.layers import MaxPool3D\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(filters=8,input_shape=(30,y,z,3),kernel_size=(3, 3, 3),padding='same', activation='relu'))\n",
    "model.add(Conv3D(filters=16,input_shape=(30,y,z,3),kernel_size=(3, 3, 3),padding='same', activation='relu'))\n",
    "model.add(Conv3D(filters=32,input_shape=(30,y,z,3),kernel_size=(3, 3, 3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Conv3D(filters=64,input_shape=(30,y,z,3),kernel_size=(3, 3, 3),padding='same', activation='relu'))\n",
    "model.add(Conv3D(filters=128,input_shape=(30,y,z,3),kernel_size=(3, 3, 3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## input layer\n",
    "# input_layer = Input((16, 16, 16, 3))\n",
    "\n",
    "# ## convolutional layers\n",
    "# conv_layer1 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
    "# conv_layer2 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
    "\n",
    "# ## add max pooling to obtain the most imformatic features\n",
    "# pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
    "\n",
    "# conv_layer3 = Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
    "# conv_layer4 = Conv3D(filters=512, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
    "# pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
    "\n",
    "# ## perform batch normalization on the convolution outputs before feeding it to MLP architecture\n",
    "# pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
    "# flatten_layer = Flatten()(pooling_layer2)\n",
    "\n",
    "# ## create an MLP architecture with dense layers : 4096 -> 512 -> 10\n",
    "# ## add dropouts to avoid overfitting / perform regularization\n",
    "# dense_layer1 = Dense(units=1024, activation='relu')(flatten_layer)\n",
    "# dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "# dense_layer2 = Dense(units=256, activation='relu')(dense_layer1)\n",
    "# dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "# output_layer = Dense(units=10, activation='softmax')(dense_layer2)\n",
    "\n",
    "# ## define the model with input layer and output layer\n",
    "# model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_34 (Conv3D)           (None, 30, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 30, 100, 100, 16)  3472      \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 30, 100, 100, 32)  13856     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 15, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 15, 50, 50, 64)    55360     \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 15, 50, 50, 128)   221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 7, 25, 25, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 560000)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              573441024 \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 573,999,365\n",
      "Trainable params: 573,999,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = 0.1\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                     validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
